{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a396458-7cb0-449a-b518-43390b45f61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71a06fee-84c8-4a68-8f5f-8ac182344c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-01-02</th>\n",
       "      <td>214.399994</td>\n",
       "      <td>216.160004</td>\n",
       "      <td>213.979996</td>\n",
       "      <td>216.160004</td>\n",
       "      <td>209.638062</td>\n",
       "      <td>30969400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-03</th>\n",
       "      <td>213.300003</td>\n",
       "      <td>215.470001</td>\n",
       "      <td>213.279999</td>\n",
       "      <td>214.179993</td>\n",
       "      <td>207.717773</td>\n",
       "      <td>27518900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-06</th>\n",
       "      <td>212.500000</td>\n",
       "      <td>215.589996</td>\n",
       "      <td>212.240005</td>\n",
       "      <td>215.559998</td>\n",
       "      <td>209.056168</td>\n",
       "      <td>21655300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-07</th>\n",
       "      <td>215.639999</td>\n",
       "      <td>216.139999</td>\n",
       "      <td>214.850006</td>\n",
       "      <td>215.529999</td>\n",
       "      <td>209.027039</td>\n",
       "      <td>22139300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-08</th>\n",
       "      <td>215.500000</td>\n",
       "      <td>218.139999</td>\n",
       "      <td>215.160004</td>\n",
       "      <td>217.149994</td>\n",
       "      <td>210.598160</td>\n",
       "      <td>26397300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-28</th>\n",
       "      <td>515.219971</td>\n",
       "      <td>523.000000</td>\n",
       "      <td>511.779999</td>\n",
       "      <td>521.809998</td>\n",
       "      <td>521.809998</td>\n",
       "      <td>33194200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-29</th>\n",
       "      <td>522.460022</td>\n",
       "      <td>522.590027</td>\n",
       "      <td>516.900024</td>\n",
       "      <td>520.830017</td>\n",
       "      <td>520.830017</td>\n",
       "      <td>26649000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-30</th>\n",
       "      <td>523.710022</td>\n",
       "      <td>526.099976</td>\n",
       "      <td>518.210022</td>\n",
       "      <td>523.049988</td>\n",
       "      <td>523.049988</td>\n",
       "      <td>27431300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-31</th>\n",
       "      <td>526.919983</td>\n",
       "      <td>531.520020</td>\n",
       "      <td>521.190002</td>\n",
       "      <td>522.289978</td>\n",
       "      <td>522.289978</td>\n",
       "      <td>38845500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-02-03</th>\n",
       "      <td>513.469971</td>\n",
       "      <td>520.849976</td>\n",
       "      <td>511.049988</td>\n",
       "      <td>518.109985</td>\n",
       "      <td>518.109985</td>\n",
       "      <td>40186900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1279 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Open        High         Low       Close   Adj Close  \\\n",
       "Date                                                                     \n",
       "2020-01-02  214.399994  216.160004  213.979996  216.160004  209.638062   \n",
       "2020-01-03  213.300003  215.470001  213.279999  214.179993  207.717773   \n",
       "2020-01-06  212.500000  215.589996  212.240005  215.559998  209.056168   \n",
       "2020-01-07  215.639999  216.139999  214.850006  215.529999  209.027039   \n",
       "2020-01-08  215.500000  218.139999  215.160004  217.149994  210.598160   \n",
       "...                ...         ...         ...         ...         ...   \n",
       "2025-01-28  515.219971  523.000000  511.779999  521.809998  521.809998   \n",
       "2025-01-29  522.460022  522.590027  516.900024  520.830017  520.830017   \n",
       "2025-01-30  523.710022  526.099976  518.210022  523.049988  523.049988   \n",
       "2025-01-31  526.919983  531.520020  521.190002  522.289978  522.289978   \n",
       "2025-02-03  513.469971  520.849976  511.049988  518.109985  518.109985   \n",
       "\n",
       "              Volume  \n",
       "Date                  \n",
       "2020-01-02  30969400  \n",
       "2020-01-03  27518900  \n",
       "2020-01-06  21655300  \n",
       "2020-01-07  22139300  \n",
       "2020-01-08  26397300  \n",
       "...              ...  \n",
       "2025-01-28  33194200  \n",
       "2025-01-29  26649000  \n",
       "2025-01-30  27431300  \n",
       "2025-01-31  38845500  \n",
       "2025-02-03  40186900  \n",
       "\n",
       "[1279 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fetch data\n",
    "today=datetime.today().strftime('%Y-%m-%d')\n",
    "data = yf.download('QQQ', start='2020-01-01', end=today)\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09614b74-4eb7-4f7f-8d3f-48dd77d94a3b",
   "metadata": {},
   "source": [
    "# create target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e4fc473-9adf-456b-a7e2-542e02da4f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# % chagne from prior close\n",
    "data[\"daily_change_%\"]=data[\"Close\"].pct_change()*100\n",
    "# % chagne current open-close\n",
    "data[\"curday_change_%\"]=(data[\"Close\"]-data[\"Open\"])/data[\"Open\"]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "783328ea-41ec-44e3-98c4-c26cff69aa7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>daily_change_%</th>\n",
       "      <th>curday_change_%</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-01-02</th>\n",
       "      <td>214.399994</td>\n",
       "      <td>216.160004</td>\n",
       "      <td>213.979996</td>\n",
       "      <td>216.160004</td>\n",
       "      <td>209.638062</td>\n",
       "      <td>30969400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.820900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-03</th>\n",
       "      <td>213.300003</td>\n",
       "      <td>215.470001</td>\n",
       "      <td>213.279999</td>\n",
       "      <td>214.179993</td>\n",
       "      <td>207.717773</td>\n",
       "      <td>27518900</td>\n",
       "      <td>-0.915993</td>\n",
       "      <td>0.412560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-06</th>\n",
       "      <td>212.500000</td>\n",
       "      <td>215.589996</td>\n",
       "      <td>212.240005</td>\n",
       "      <td>215.559998</td>\n",
       "      <td>209.056168</td>\n",
       "      <td>21655300</td>\n",
       "      <td>0.644320</td>\n",
       "      <td>1.439999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-07</th>\n",
       "      <td>215.639999</td>\n",
       "      <td>216.139999</td>\n",
       "      <td>214.850006</td>\n",
       "      <td>215.529999</td>\n",
       "      <td>209.027039</td>\n",
       "      <td>22139300</td>\n",
       "      <td>-0.013917</td>\n",
       "      <td>-0.051011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-08</th>\n",
       "      <td>215.500000</td>\n",
       "      <td>218.139999</td>\n",
       "      <td>215.160004</td>\n",
       "      <td>217.149994</td>\n",
       "      <td>210.598160</td>\n",
       "      <td>26397300</td>\n",
       "      <td>0.751633</td>\n",
       "      <td>0.765658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-28</th>\n",
       "      <td>515.219971</td>\n",
       "      <td>523.000000</td>\n",
       "      <td>511.779999</td>\n",
       "      <td>521.809998</td>\n",
       "      <td>521.809998</td>\n",
       "      <td>33194200</td>\n",
       "      <td>1.477991</td>\n",
       "      <td>1.279071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-29</th>\n",
       "      <td>522.460022</td>\n",
       "      <td>522.590027</td>\n",
       "      <td>516.900024</td>\n",
       "      <td>520.830017</td>\n",
       "      <td>520.830017</td>\n",
       "      <td>26649000</td>\n",
       "      <td>-0.187804</td>\n",
       "      <td>-0.311987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-30</th>\n",
       "      <td>523.710022</td>\n",
       "      <td>526.099976</td>\n",
       "      <td>518.210022</td>\n",
       "      <td>523.049988</td>\n",
       "      <td>523.049988</td>\n",
       "      <td>27431300</td>\n",
       "      <td>0.426237</td>\n",
       "      <td>-0.126030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-31</th>\n",
       "      <td>526.919983</td>\n",
       "      <td>531.520020</td>\n",
       "      <td>521.190002</td>\n",
       "      <td>522.289978</td>\n",
       "      <td>522.289978</td>\n",
       "      <td>38845500</td>\n",
       "      <td>-0.145303</td>\n",
       "      <td>-0.878692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-02-03</th>\n",
       "      <td>513.469971</td>\n",
       "      <td>520.849976</td>\n",
       "      <td>511.049988</td>\n",
       "      <td>518.109985</td>\n",
       "      <td>518.109985</td>\n",
       "      <td>40186900</td>\n",
       "      <td>-0.800320</td>\n",
       "      <td>0.903658</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1279 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Open        High         Low       Close   Adj Close  \\\n",
       "Date                                                                     \n",
       "2020-01-02  214.399994  216.160004  213.979996  216.160004  209.638062   \n",
       "2020-01-03  213.300003  215.470001  213.279999  214.179993  207.717773   \n",
       "2020-01-06  212.500000  215.589996  212.240005  215.559998  209.056168   \n",
       "2020-01-07  215.639999  216.139999  214.850006  215.529999  209.027039   \n",
       "2020-01-08  215.500000  218.139999  215.160004  217.149994  210.598160   \n",
       "...                ...         ...         ...         ...         ...   \n",
       "2025-01-28  515.219971  523.000000  511.779999  521.809998  521.809998   \n",
       "2025-01-29  522.460022  522.590027  516.900024  520.830017  520.830017   \n",
       "2025-01-30  523.710022  526.099976  518.210022  523.049988  523.049988   \n",
       "2025-01-31  526.919983  531.520020  521.190002  522.289978  522.289978   \n",
       "2025-02-03  513.469971  520.849976  511.049988  518.109985  518.109985   \n",
       "\n",
       "              Volume  daily_change_%  curday_change_%  \n",
       "Date                                                   \n",
       "2020-01-02  30969400             NaN         0.820900  \n",
       "2020-01-03  27518900       -0.915993         0.412560  \n",
       "2020-01-06  21655300        0.644320         1.439999  \n",
       "2020-01-07  22139300       -0.013917        -0.051011  \n",
       "2020-01-08  26397300        0.751633         0.765658  \n",
       "...              ...             ...              ...  \n",
       "2025-01-28  33194200        1.477991         1.279071  \n",
       "2025-01-29  26649000       -0.187804        -0.311987  \n",
       "2025-01-30  27431300        0.426237        -0.126030  \n",
       "2025-01-31  38845500       -0.145303        -0.878692  \n",
       "2025-02-03  40186900       -0.800320         0.903658  \n",
       "\n",
       "[1279 rows x 8 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de5b3b8-14f6-449e-86d8-41d3e099ebf4",
   "metadata": {},
   "source": [
    "# Data Preprocessing and Preparation\n",
    "handling missing values, normalizing or scaling data, and potentially creating additional features, like moving averages or percentage changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "393b4ae1-c2f1-4a53-9461-e6ac49d7b1d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Open         0\n",
       "High         0\n",
       "Low          0\n",
       "Close        0\n",
       "Adj Close    0\n",
       "Volume       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for missing values\n",
    "data.isnull().sum()\n",
    "\n",
    "# Filling missing values, it takes the last non-missing value from the previous row and fills it in.\n",
    "# data.fillna(method='ffill', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bd4bcd-336e-49a9-b542-3cb73085a505",
   "metadata": {},
   "source": [
    "use scaler if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "179bcb4a-fb0e-41d7-a3f2-f70815e67532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.12703663],\n",
       "       [0.12166886],\n",
       "       [0.12541003],\n",
       "       ...,\n",
       "       [0.95900996],\n",
       "       [0.95694959],\n",
       "       [0.9456177 ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# scaler = MinMaxScaler(feature_range=(0,1))\n",
    "# data_scaled = scaler.fit_transform(data['Close'].values.reshape(-1,1))\n",
    "# data_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1bbe869-a433-4ed5-ad6b-1f0d5539fd0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.8209001 ],\n",
       "       [ 0.41255959],\n",
       "       [ 1.43999885],\n",
       "       ...,\n",
       "       [-0.12603047],\n",
       "       [-0.87869222],\n",
       "       [ 0.90365842]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Keras & Scikit-learn expect 2D arrays for training\n",
    "data_scaled=data[\"curday_change_%\"].values.reshape(-1,1)\n",
    "data_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7b46410-1647-43af-9686-c0401289bc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting x and y, train_test\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in range(60, len(data_scaled)):\n",
    "    X.append(data_scaled[i-60:i, 0])\n",
    "    y.append(data_scaled[i, 0])\n",
    "#x as feature- 60 prious data points? y- the next day price close as the target?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "237ebabb-add5-468d-8ffb-c2c8155005a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(X) * 0.8)\n",
    "test_size = len(X) - train_size\n",
    "\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5147d3e-f4a2-4c0a-8ef9-720b1cbb7b9c",
   "metadata": {},
   "source": [
    "# Reshaping Data for LSTM\n",
    "reshape our data into a 3D format [samples, time steps, features] required by LSTM layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f387f3e-a6d2-4142-948b-9ce89eee5944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.shape[0]: Represents the number of samples in the dataset, often referred to as the batch size.\n",
    "# X_train.shape[1]: Denotes the number of time steps in each sample.\n",
    "# 1: Indicates that there is a single feature per time step, only close were selected\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50253a4-8250-4a74-bbbb-c5f6220f1001",
   "metadata": {},
   "source": [
    "# Building the LSTM with Attention Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35718654-aa9d-4dde-ada8-9169d921f70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chenl\\anaconda3\\envs\\notebook\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "#Creating LSTM Layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, AdditiveAttention, Permute, Reshape, Multiply, Flatten, Lambda, Layer,BatchNormalization\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Adding LSTM layers with return_sequences=True,stacked LSTM architecture, 2 layers\n",
    "# return_sequences=True is crucial in the first layers to ensure the output includes sequences\n",
    "#LSTM layer outputs data in shape (batch_size, timesteps, features\n",
    "model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "model.add(LSTM(units=50, return_sequences=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2aa733-5686-4a9f-b3fa-4afd7e4e2cfc",
   "metadata": {},
   "source": [
    "# Integrating the Attention Mechanism\n",
    "enhance the model’s ability to focus on relevant time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d96b866-1a02-4079-90e4-ea85c6b7a945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\chenl\\anaconda3\\envs\\notebook\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:216: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">10,400</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">20,200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ attention_layer                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)         │            <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AttentionLayer</span>)                │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3000</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3000</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3000</span>)           │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,000</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,001</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m50\u001b[0m)         │        \u001b[38;5;34m10,400\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m50\u001b[0m)         │        \u001b[38;5;34m20,200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ attention_layer                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m50\u001b[0m)         │            \u001b[38;5;34m50\u001b[0m │\n",
       "│ (\u001b[38;5;33mAttentionLayer\u001b[0m)                │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3000\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3000\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3000\u001b[0m)           │        \u001b[38;5;34m12,000\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │         \u001b[38;5;34m3,001\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">45,651</span> (178.32 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m45,651\u001b[0m (178.32 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">39,651</span> (154.89 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m39,651\u001b[0m (154.89 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,000</span> (23.44 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m6,000\u001b[0m (23.44 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Custom Attention Layer (Fix for Lambda Issue)\n",
    "class AttentionLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "        self.attention = AdditiveAttention()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        attention_output = self.attention([inputs, inputs])  # Self-attention\n",
    "        return Multiply()([inputs, attention_output])\n",
    "\n",
    "# Define Model\n",
    "model = Sequential()\n",
    "\n",
    "# First LSTM Layer\n",
    "model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "\n",
    "# Second LSTM Layer\n",
    "model.add(LSTM(units=50, return_sequences=True))\n",
    "\n",
    "# Apply Custom Attention Layer\n",
    "model.add(AttentionLayer())\n",
    "\n",
    "# Flatten Before Final Dense Layer\n",
    "model.add(Flatten())\n",
    "\n",
    "# Add Dropout for Regularization\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Add Batch Normalization\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Final Dense Layer for Stock Price Prediction, 1 means next day price, change to x if want predict x days in future\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the Model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Show Model Summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195fcad8-b195-44fd-8f79-2b99163c7033",
   "metadata": {},
   "source": [
    "Total params (33,601) → The total number of parameters (weights + biases) in your model.\n",
    "\n",
    "\n",
    "Trainable params (33,601) → These are the parameters that get updated during training (i.e., all layers are trainable).\n",
    "\n",
    "\n",
    "Non-trainable params (0) → There are no frozen layers (e.g., pre-trained layers from another model)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c49c80-7e25-4a63-8133-84277024be9c",
   "metadata": {},
   "source": [
    "# Training the Model\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ebf846c8-6e5d-4ee7-aa44-523c1d3a838d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 85ms/step - loss: 1.6573 - val_loss: 0.7177\n",
      "Epoch 2/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 1.7613 - val_loss: 0.7226\n",
      "Epoch 3/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 1.7058 - val_loss: 0.7213\n",
      "Epoch 4/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - loss: 1.5238 - val_loss: 0.7274\n",
      "Epoch 5/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - loss: 1.7582 - val_loss: 0.7185\n",
      "Epoch 6/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - loss: 1.7529 - val_loss: 0.7171\n",
      "Epoch 7/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 59ms/step - loss: 1.6628 - val_loss: 0.7179\n",
      "Epoch 8/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 57ms/step - loss: 1.7945 - val_loss: 0.7503\n",
      "Epoch 9/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 59ms/step - loss: 1.7056 - val_loss: 0.7176\n",
      "Epoch 10/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 59ms/step - loss: 1.6699 - val_loss: 0.7238\n",
      "Epoch 11/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 57ms/step - loss: 1.4350 - val_loss: 0.7538\n",
      "Epoch 12/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 57ms/step - loss: 1.6019 - val_loss: 0.7135\n",
      "Epoch 13/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 59ms/step - loss: 1.4644 - val_loss: 0.7192\n",
      "Epoch 14/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 57ms/step - loss: 1.4622 - val_loss: 0.7141\n",
      "Epoch 15/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - loss: 1.6268 - val_loss: 0.7258\n",
      "Epoch 16/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 57ms/step - loss: 1.5571 - val_loss: 0.7132\n",
      "Epoch 17/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 57ms/step - loss: 1.3580 - val_loss: 0.7234\n",
      "Epoch 18/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - loss: 1.4709 - val_loss: 0.7219\n",
      "Epoch 19/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - loss: 1.5058 - val_loss: 0.7230\n",
      "Epoch 20/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - loss: 1.4173 - val_loss: 0.7202\n",
      "Epoch 21/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 59ms/step - loss: 1.5958 - val_loss: 0.7225\n",
      "Epoch 22/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 59ms/step - loss: 1.5090 - val_loss: 0.7346\n",
      "Epoch 23/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 59ms/step - loss: 1.5504 - val_loss: 0.7240\n",
      "Epoch 24/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - loss: 1.4723 - val_loss: 0.7321\n",
      "Epoch 25/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 59ms/step - loss: 1.5087 - val_loss: 0.7344\n",
      "Epoch 26/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - loss: 1.3963 - val_loss: 0.7618\n",
      "Epoch 27/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 66ms/step - loss: 1.3767 - val_loss: 0.7737\n",
      "Epoch 28/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 59ms/step - loss: 1.4632 - val_loss: 0.7982\n",
      "Epoch 29/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 1.4399 - val_loss: 0.7588\n",
      "Epoch 30/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - loss: 1.2826 - val_loss: 0.7418\n",
      "Epoch 31/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - loss: 1.4018 - val_loss: 0.7593\n",
      "Epoch 32/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step - loss: 1.4266 - val_loss: 0.7456\n",
      "Epoch 33/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 1.2275 - val_loss: 0.7897\n",
      "Epoch 34/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 1.3521 - val_loss: 0.8113\n",
      "Epoch 35/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 1.2438 - val_loss: 0.7669\n",
      "Epoch 36/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 1.4306 - val_loss: 0.7730\n",
      "Epoch 37/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 1.2564 - val_loss: 0.7728\n",
      "Epoch 38/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 1.3295 - val_loss: 0.8784\n",
      "Epoch 39/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 1.1290 - val_loss: 0.9280\n",
      "Epoch 40/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - loss: 1.2505 - val_loss: 0.8790\n",
      "Epoch 41/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - loss: 1.0859 - val_loss: 0.8694\n",
      "Epoch 42/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - loss: 1.1063 - val_loss: 1.0202\n",
      "Epoch 43/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - loss: 1.1857 - val_loss: 0.9604\n",
      "Epoch 44/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - loss: 0.9841 - val_loss: 0.9713\n",
      "Epoch 45/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - loss: 1.0115 - val_loss: 1.1352\n",
      "Epoch 46/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - loss: 0.9748 - val_loss: 1.0070\n",
      "Epoch 47/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - loss: 1.0294 - val_loss: 0.9719\n",
      "Epoch 48/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - loss: 0.9364 - val_loss: 0.9273\n",
      "Epoch 49/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - loss: 0.9003 - val_loss: 0.9808\n",
      "Epoch 50/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - loss: 0.9387 - val_loss: 0.9084\n",
      "Epoch 51/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - loss: 0.9208 - val_loss: 0.9491\n",
      "Epoch 52/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - loss: 0.8289 - val_loss: 1.1962\n",
      "Epoch 53/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - loss: 0.8783 - val_loss: 1.0817\n",
      "Epoch 54/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - loss: 0.8144 - val_loss: 1.0616\n",
      "Epoch 55/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - loss: 0.7475 - val_loss: 1.2071\n",
      "Epoch 56/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - loss: 0.8098 - val_loss: 1.2249\n",
      "Epoch 57/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - loss: 0.8423 - val_loss: 1.3610\n",
      "Epoch 58/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - loss: 0.7725 - val_loss: 1.2880\n",
      "Epoch 59/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - loss: 0.7284 - val_loss: 1.1325\n",
      "Epoch 60/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - loss: 0.7314 - val_loss: 1.2857\n",
      "Epoch 61/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - loss: 0.7196 - val_loss: 1.2533\n",
      "Epoch 62/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - loss: 0.6728 - val_loss: 1.4585\n",
      "Epoch 63/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - loss: 0.7443 - val_loss: 1.4197\n",
      "Epoch 64/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - loss: 0.8043 - val_loss: 1.5796\n",
      "Epoch 65/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 59ms/step - loss: 0.7844 - val_loss: 1.3890\n",
      "Epoch 66/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - loss: 0.7138 - val_loss: 1.3956\n",
      "Epoch 67/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - loss: 0.5800 - val_loss: 1.3606\n",
      "Epoch 68/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - loss: 0.6073 - val_loss: 1.5283\n",
      "Epoch 69/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - loss: 0.5413 - val_loss: 1.5006\n",
      "Epoch 70/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - loss: 0.6129 - val_loss: 1.3078\n",
      "Epoch 71/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - loss: 0.5871 - val_loss: 1.4759\n",
      "Epoch 72/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - loss: 0.5635 - val_loss: 1.6710\n",
      "Epoch 73/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - loss: 0.5811 - val_loss: 1.3189\n",
      "Epoch 74/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - loss: 0.5196 - val_loss: 1.4431\n",
      "Epoch 75/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 0.5392 - val_loss: 1.2833\n",
      "Epoch 76/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - loss: 0.4860 - val_loss: 1.7423\n",
      "Epoch 77/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - loss: 0.5238 - val_loss: 1.5841\n",
      "Epoch 78/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - loss: 0.4851 - val_loss: 1.3862\n",
      "Epoch 79/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - loss: 0.5859 - val_loss: 1.5895\n",
      "Epoch 80/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - loss: 0.5119 - val_loss: 1.5106\n",
      "Epoch 81/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - loss: 0.4293 - val_loss: 1.6773\n",
      "Epoch 82/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - loss: 0.4990 - val_loss: 1.5551\n",
      "Epoch 83/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - loss: 0.4368 - val_loss: 1.4535\n",
      "Epoch 84/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - loss: 0.4656 - val_loss: 1.5181\n",
      "Epoch 85/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - loss: 0.4499 - val_loss: 1.4817\n",
      "Epoch 86/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - loss: 0.4756 - val_loss: 1.7046\n",
      "Epoch 87/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - loss: 0.3831 - val_loss: 1.6159\n",
      "Epoch 88/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 0.4183 - val_loss: 1.6463\n",
      "Epoch 89/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - loss: 0.4022 - val_loss: 1.7013\n",
      "Epoch 90/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - loss: 0.3641 - val_loss: 1.4520\n",
      "Epoch 91/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - loss: 0.5300 - val_loss: 1.5036\n",
      "Epoch 92/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - loss: 0.3391 - val_loss: 1.6522\n",
      "Epoch 93/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - loss: 0.3910 - val_loss: 1.7010\n",
      "Epoch 94/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - loss: 0.3323 - val_loss: 1.5645\n",
      "Epoch 95/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - loss: 0.3518 - val_loss: 1.5721\n",
      "Epoch 96/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - loss: 0.3908 - val_loss: 1.4650\n",
      "Epoch 97/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - loss: 0.3793 - val_loss: 1.6751\n",
      "Epoch 98/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - loss: 0.4078 - val_loss: 1.7907\n",
      "Epoch 99/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - loss: 0.3231 - val_loss: 1.5237\n",
      "Epoch 100/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - loss: 0.4135 - val_loss: 1.6709\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=100, batch_size=25, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a089ad79-be6e-46e5-b3ac-b427915f2d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.callbacks import EarlyStopping\n",
    "# # stops training when the model’s performance on the validation set starts to degrade.\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "# history = model.fit(X_train, y_train, epochs=100, batch_size=25, validation_split=0.2, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c550508-2019-4331-9f31-ecd4fed8c723",
   "metadata": {},
   "source": [
    "\n",
    "# option ohter call back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f706bf47-22a8-4e0f-8b44-c7245e400c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, TensorBoard, CSVLogger\n",
    "\n",
    "# # Callback to save the model periodically\n",
    "# model_checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss')\n",
    "\n",
    "# # Callback to reduce learning rate when a metric has stopped improving\n",
    "# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5)\n",
    "\n",
    "# # Callback for TensorBoard\n",
    "# tensorboard = TensorBoard(log_dir='./logs')\n",
    "\n",
    "# # Callback to log details to a CSV file\n",
    "# csv_logger = CSVLogger('training_log.csv')\n",
    "\n",
    "# # Combining all callbacks\n",
    "# callbacks_list = [early_stopping, model_checkpoint, reduce_lr, tensorboard, csv_logger]\n",
    "\n",
    "# # Fit the model with the callbacks\n",
    "# history = model.fit(X_train, y_train, epochs=100, batch_size=25, validation_split=0.2, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "71d48b1c-1768-425d-9a68-708a08c0d410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 1.8678\n",
      "Test Loss:  2.2424967288970947\n"
     ]
    }
   ],
   "source": [
    "# Convert X_test and y_test to Numpy arrays if they are not already\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# Ensure X_test is reshaped similarly to how X_train was reshaped\n",
    "# This depends on how you preprocessed the training data\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Now evaluate the model on the test data\n",
    "test_loss = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss: \", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c47ed924-43da-49d5-8686-0276521216dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step\n",
      "Mean Absolute Error:  1.1273904362158826\n",
      "Root Mean Square Error:  1.497496846139528\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Making predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculating MAE and RMSE\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "\n",
    "print(\"Mean Absolute Error: \", mae)\n",
    "print(\"Root Mean Square Error: \", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0064a61e-7617-465a-b66a-64a614735f25",
   "metadata": {},
   "source": [
    "# use prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aa9ccf67-758c-442c-af05-d8b88aefd208",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Predicted Stock Prices for the next day:  [[1.1843278]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Fetching the latest 60 days of AAPL stock data\n",
    "data = yf.download('QQQ', period='60d', interval='1d')\n",
    "data[\"curday_change_%\"]=(data[\"Close\"]-data[\"Open\"])/data[\"Open\"]*100\n",
    "\n",
    "# Selecting the 'Close' price and converting to numpy array\n",
    "scaled_data = data[\"curday_change_%\"].values.reshape(-1,1)\n",
    "\n",
    "# Scaling the data\n",
    "# scaler = MinMaxScaler(feature_range=(0,1))\n",
    "# scaled_data = scaler.fit_transform(closing_prices.reshape(-1,1))\n",
    "\n",
    "# Since we need the last 60 days to predict the next day, we reshape the data accordingly\n",
    "X_latest = np.array([scaled_data[-60:].reshape(60)])\n",
    "\n",
    "# Reshaping the data for the model (adding batch dimension)\n",
    "X_latest = np.reshape(X_latest, (X_latest.shape[0], X_latest.shape[1], 1))\n",
    "\n",
    "# Making predictions for the next 4 candles\n",
    "predicted_stock_price = model.predict(X_latest)\n",
    "# predicted_stock_price = scaler.inverse_transform(predicted_stock_price)\n",
    "\n",
    "print(\"Predicted Stock next day: \", predicted_stock_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "00f931a1-5cb7-40f2-bda4-9d6bf1948afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import datetime, timedelta\n",
    "\n",
    "# def predict_stock_price(input_date):\n",
    "#     # Check if the input date is a valid date format\n",
    "#     try:\n",
    "#         input_date = pd.to_datetime(input_date)\n",
    "#     except ValueError:\n",
    "#         print(\"Invalid Date Format. Please enter date in YYYY-MM-DD format.\")\n",
    "#         return\n",
    "\n",
    "#     # Fetch data from yfinance\n",
    "#     end_date = input_date\n",
    "#     start_date = input_date - timedelta(days=90)  # Fetch more days to ensure we have 60 trading days\n",
    "#     data = yf.download('AAPL', start=start_date, end=end_date)\n",
    "\n",
    "#     if len(data) < 60:\n",
    "#         print(\"Not enough historical data to make a prediction. Try an earlier date.\")\n",
    "#         return\n",
    "\n",
    "#     # Prepare the data\n",
    "#     closing_prices = data['Close'].values[-60:]  # Last 60 days\n",
    "#     scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "#     scaled_data = scaler.fit_transform(closing_prices.reshape(-1, 1))\n",
    "\n",
    "#     # Make predictions\n",
    "#     predicted_prices = []\n",
    "#     current_batch = scaled_data.reshape(1, 60, 1)\n",
    "\n",
    "#     for i in range(4):  # Predicting 4 days\n",
    "#         next_prediction = model.predict(current_batch)\n",
    "#         next_prediction_reshaped = next_prediction.reshape(1, 1, 1)\n",
    "#         current_batch = np.append(current_batch[:, 1:, :], next_prediction_reshaped, axis=1)\n",
    "#         predicted_prices.append(scaler.inverse_transform(next_prediction)[0, 0])\n",
    "\n",
    "#     # Output the predictions\n",
    "#     for i, price in enumerate(predicted_prices, 1):\n",
    "#         print(f\"Day {i} prediction: {price}\")\n",
    "\n",
    "# # Example use\n",
    "# user_input = input(\"Enter a date (YYYY-MM-DD) to predict stock for the next 4 days: \")\n",
    "# predict_stock_price(user_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3894c3-5086-4099-9770-843de028ee74",
   "metadata": {},
   "source": [
    "# next step, \n",
    "1. input more features?\n",
    "2. create diff target: % change of the day (open to close, prior close to close), also try just classification for direction, and longer expect\n",
    "3. tuning the basic model\n",
    "4. adjust this LSTM model (the scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7113d5f-773e-40b2-87a8-cca60b72141e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
